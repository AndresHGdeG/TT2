\section{Conclusiones}

La cantidad de páginas electrónicas, para consultar noticias ha incrementado con el paso de los años, su información al igual que un diario tradicional se encuentra dividida en secciones para facilitar 
la consulta, sin embargo, la clasificación suele variar en cada portal, incluso teniendo el mismo contenido. Por esta razón, buscar artículos de interés se ha hecho una tarea complicada y laboriosa. Para abordar una solución con base en este problema, el presente trabajo ha desarrollado
una aplicación web, el cual permite recopilar noticias de diferentes fuentes y mediante un algoritmo son clasificadas automáticamente en 5 secciones diferentes.\\

Como primer punto abordado en el desarrollo de este trabajo, se debe mencionar que uno de los objetivos particulares, el cual es afinar el clasificador del trabajo terminal 2017-A042, sin embargo, debido a los diferentes enfoques de clasificación se decidió generar un nuevo modelo y corpus.\\ 

Se recolectó un corpus de 3,500 noticias, con el fin de tener información suficiente para entrenar y probar 4 algoritmos de clasificación de la biblioteca \textbf{scikit-learn} los cuales son: \textbf{Naive Bayes}, \textbf{Regresión logística}, \textbf{Random Forest} y \textbf{Máquina de soporte vectorial}. Además para medir la eficiencia de los algoritmos se ha hecho uso de la técnica validación cruzada, el cual consiste en dividir un corpus en conjunto de entrenamiento y prueba, calcular la exactitud de cada prueba y obtener el promedio total. \textbf{Máquina de soporte vectorial} ha tenido el mejor resultado en la etapa de entrenamiento, obteniendo 0.89 de exactitud en el conjunto de pruebas. Por esta razón el modelo clasificador se ha entrenado con este algoritmo. 

Para recolectar las noticias se ha utilizado la biblioteca \textbf{scrapy}, la cual permite crear arañas que extraen los elementos de las noticias (\textbf{título}, \textbf{URL}, \textbf{Contenido}, etc) de 7 sitios web. Cabe destacar que el desarrollo de ambas herramientas (modelo clasificador y recolector) se ha hecho en el lenguaje \textbf{python 3}.\\


Con todo lo desarrollado se puede concluir que la herramienta permite recolectar y clasificar las noticias con muy buena exactitud y es una mejora con respecto a las herramientas existentes ya que la clasificación se basa en su contenido y no en etiquetas, además que permite definir filtros de secciones y periodos de fecha. Por lo anterior se han cumplido todos los objetivos planteados en este trabajo.


%El presente trabajo evaluó las técnicas existentes para la recolección de información de sitios %web, así como la representación de la información recolectada, la selección de características, %para finalmente realizar pruebas con los algoritmos de clasificación mencionados, con el fin de %obtener el mejor algoritmo que se adecue a nuestro objetivo, el cual es clasificar noticias.%
%
%Las consideraciones en el corpus utilizado, nos permitió obtener mejores resultados, ya que se %omitieron palabras, símbolos así como se valido la longitud de la noticia, lo cual nos podía %arrojar una clasificación errónea.

%
%1 Objetivo general
%2 Objetivos particulares
%3 Recolección de noticia
%4 Clasificación de noticia
%5 Resultados%