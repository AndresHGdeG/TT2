\section[Entrenamiento]{Entrenamiento de clasificador}

El segundo pilar del trabajo terminal es entrenar un algoritmo (con aprendizaje supervisado), para clasificar las noticias en las secciones definidas en el capitulo 4 (ver). Cabe destacar que el clasificador resolverá un problema multiclase (ver) debido a que la entrada es una artículo y como salida brinda la pertenencia a una sección  de 5 posibilidades. El proceso de entrenamiento se muestra en la Figura \ref{fig:cp5:procesoE}.\\

Como se mencionó en la etapa de recolección, el corpus ocupado contiene 700 noticias por cada sección es decir, 3500 artículos como total del dataset. Sin embargo solo el 90\% de este será ocupado en la etapa de entrenamiento y el 10\% restante se ocupará para hacer la etapa de prueba, teniendo así un total de 3,150 noticias de entrenamiento y 350 para medir la eficiencia del clasificador, estos conjuntos serán definidos al final de la etapa de preprocesamiento.\\

\begin{figure}[h]
\centering
\includegraphics[scale=.55]{imagenes/capitulo5/Esquema.png}
\caption{Proceso de entrenamiento}
\label{fig:cp5:procesoE}
\end{figure}


%------------------------------------------------------------------%
\subsection{Preprocesamiento}

Como primera instancia, el corpus creado de 3500 noticias debe ser procesado, con el fin de crear vectores que representan el contenido de cada artículo de forma ordenada(ver), de esta manera los algoritmos de clasificación son capaces de entender la información. En cuanto a los datos que son procesados de la noticia cabe mencionar que solo se usa el título y la redacción del artículo, los demás datos(como url, fecha, sección) no son necesarios para el entrenamiento. Este proceso consta de 6 etapas, las cuales son mostradas en la Figura \ref{fig:cp5:preprocesamiento}. \\

Cabe destacar que estas etapas están desarrolladas en un script escrito en el lenguaje \textbf{python 3}, en cada sección se hará mención de las librerías ocupadas.\\

\begin{figure}[h]
\centering
\includegraphics[scale=.55]{imagenes/capitulo5/preprocesamiento.png}
\caption{Etapas de preprocesamiento}
\label{fig:cp5:preprocesamiento}
\end{figure}

%----------------------------------%
\begin{large}
\textbf{Limpieza}\\
\end{large}

Esta etapa consiste en eliminar texto que no brinda información útil para el entrenamiento como, hipertexto (ver), símbolos especiales (como \# $\dagger$ $\sqrt{ }$), \textit{emojis} (como \dSmiley \dCooley \dNinja). Por ejemplo el texto \ref{box:cp5:texto} muestra la redacción de una noticia con la información descargada de una pagina web. El resultado de limpiar la noticia se muestra en el texto \ref{box:cp5:limpio}. Se puede observar que se han eliminado los elementos $<p>\ </p>\ <!--\ -->$ \# $\dagger$ \dSmiley \dCooley \dInnocey.\\

Para realizar esta tarea se han ocupando las librerías; \textbf{pandas} (ver) como medio de lectura de archivos tipo \textbf{CSV} (\textit{comma separeted values}, por sus siglas en ingles); \textbf{re} (ver) la cual permite evaluar expresiones regulares con el objetivo de eliminar símbolos; \textbf{demoji} (ver) quien permite eliminar \textit{emojis} dentro del texto.\\

\begin{mygraybox}[label={box:cp5:texto}]{Texto de entrada} 
$<p>\dagger$$\ \ \ $ El número 343 de El Trimestre Económico,$\otimes$ revista emblemática del Fondo de Cultura 
$<!--\ -->$
Económica (FCE), será * * * presentado por David Ibarra Muñoz \dSmiley , Carlos Tello Macías \dCooley , Alicia Puyana \dInnocey y Pablo Ruiz Nápoles el martes 27 de agosto a las 6 de la tarde, en la librería Rosario Castellanos, ubicada en avenida Tamaulipas \# 202, en la colonia Condesa de la capital mexicana.$</p>$
\end{mygraybox}

\ \\

\begin{mygraybox}[label={box:cp5:limpio}]{Texto limpio} 
El número 343 de El Trimestre Económico, revista emblemática del Fondo de Cultura 
Económica (FCE), será presentado por David Ibarra Muñoz, Carlos Tello Macías , Alicia Puyana y Pablo Ruiz Nápoles el martes 27 de agosto a las 6 de la tarde, en la librería Rosario Castellanos, ubicada en avenida Tamaulipas 202, en la colonia Condesa de la capital mexicana.
\end{mygraybox}
\ \\

%----------------------------------%
\begin{large}
\textbf{Filtro de longitud}\\
\end{large}

Con base en la regla de negocio (ver) se ha definido 180 palabras como longitud mínima de las noticias, incluyendo en la definición de palabra números, signos de puntuación y exclamación.
Para esto después de haber concluido el proceso de limpieza se pregunta por la longitud de la cadena (sin contar los espacios) y si esta es mayor o igual a 180, entonces es un artículo válido para utilizar en el entrenamiento de lo contrario no es tomado en cuenta.\\

%----------------------------------%
\begin{large}
\textbf{Tokenizar}\\
\end{large}

La etapa de tokenización consiste en separar el texto en sus elementos mínimos llamados tokens, donde se separan palabras, signos de puntuación, llaves y números mediante un espacio. Continuando con el ejemplo \ref{box:cp5:limpio} donde el texto se encuentra limpio, se procede a su tokenización. El resultado es mostrado en el Cuadro \ref{box:cp5:tokenizar}. Para remarcar el ejemplo observe la palabra entre paréntesis \textbf{(FCE)} la cual es separada en \textbf{(FCE)} mostrando que ahora cada elemento representa un token individual. Para el desarrollo de esta tarea se ocupo la librería \textbf{RegexpTokenizer}.\\

\begin{mygraybox}[label={box:cp5:tokenizar}]{Texto tokenizado} 
El número 343 de El Trimestre Económico , revista emblemática del Fondo de Cultura 
Económica (FCE) , será presentado por David Ibarra Muñoz , Carlos Tello Macías , Alicia Puyana y Pablo Ruiz Nápoles el martes 27 de agosto a las 6 de la tarde , en la librería Rosario Castellanos , ubicada en avenida Tamaulipas 202 , en la colonia Condesa de la capital mexicana .
\end{mygraybox}
\ \\
%----------------------------------%
\begin{large}
\textbf{Lematizar}\\
\end{large}

Lematizar es el proceso de reducir cada palabra a su lemma, con el fin de eliminar ruido en el texto, por ejemplo las palabras correrás, corriendo, corrí, tienen como lema el verbo correr, el plural niños tiene como lema niño (ver). Para realizar esta tarea se ha usado \textbf{spacy} (ver) el cual es una librería de código abierto, con el diccionario \textit{es\_core\_news\_sm} quien permite analizar el léxico del lenguaje español.\\

Siguiendo con las etapas del proceso se toma el texto \ref{box:cp5:tokenizar} como entrada al programa y este da como salida el texto que se muestra en el Cuadro \ref{box:cp5:lematizado}.\\

\begin{mygraybox}[label={box:cp5:lematizado}]{Texto lematizado} 
el número 343 de el trimestre económico , revista emblemático del fondo de cultura económica (fce) , ser presentar por david ibarra muñoz , carlos tello macías , alicia puyana y pablo ruiz nápoles el martes 27 de agostar a los 6 de lo tardar , en lo librería rosario castellanos , ubicar en avenir tamaulipas 202 , en lo colonia condesa de lo capital mexicano .
\end{mygraybox}
\ \\

Cuando el proceso de lematización concluye se genera un identificador único para cada noticia el cual se define de la siguiente forma 

\begin{equation*}
id=<Identificador\ de\ sitio\ web><Numero\ de\ noticias>
\end{equation*}
 
donde \textbf{Identificador de sitio web} define un número único para hacer referencia a los sitios web (ver Tabla \ref{tab:cp5:sitioweb}) y  \textbf{Número de noticias} es el número del artículo.

\begin{table}[h]
\centering
	\begin{tabular}{|l|l|}
	%-----------------------Ecanbezado-----------------------------------%
		\hline
		\multicolumn{1}{| >{\columncolor{white!30!black}}l|}{ \textcolor{myWhite}{\textbf{Número}} }&
		\multicolumn{1}{| >{\columncolor{white!30!black}}l|}{ \textcolor{myWhite}{\textbf{Página web}} }\\
		\hline
	%------------------------------------------------%

		\textbf{100} & Aristegui noticias\\
		\hline
		\textbf{200} & Tv azteca\\
		\hline

		\textbf{300} & El Economista\\
		\hline

		\textbf{400} & La jornada\\	
		\hline

		\textbf{500} & La prensa\\
		\hline

		\textbf{600} & Proceso\\
		\hline

		\textbf{700} & Sopitas\\
		\hline

	\end{tabular}\\
\caption{Identificador de sito web}
\label{tab:cp5:sitioweb}
\end{table}


Como segundo paso las noticias son almacenadas en un archivo con extensión \textbf{TXT}, los elementos por almacenar son \textbf{id}, \textbf{titulo}, \textbf{noticia} y \textbf{sección}, los cuales son separados por los caracteres \&\&\&\&\& . El Cuadro \ref{box:cp5:archivo} muestra un ejemplo de la estructura del archivo.\\

\begin{mygraybox}[label={box:cp5:archivo}]{Estructura de archivo} 
id\&\&\&\&\&titulo\&\&\&\&\&noticia\&\&\&\&\&seccion\\
1001\&\&\&\&\&Titulo 1\&\&\&\&\&Contenido noticia 1\&\&\&\&\&0\\
...\\
5003500\&\&\&\&\&Titulo 3500\&\&\&\&\&Contenido noticia 3500\&\&\&\&\&4
\end{mygraybox}

\ \\

%----------------------------------%

\begin{large}
\textbf{División del corpus}\\
\end{large}


Para el correcto diseño y evaluación del algoritmos se requiere dividir el corpus en dos conjuntos: \textbf{entrenamiento} y \textbf{prueba}, con un 90\% y 10\% del total del \textit{dataset} respectivamente. En cada grupo deben estar repartidas noticias de las 5 secciones definidas, sin embargo los artículos almacenados están ordenados de forma descendente como: \textbf{Deportes}, \textbf{Economía}, \textbf{Política}, \textbf{Cultura}, \textbf{Ciencia y tecnología}, para seleccionar deforma distribuida los datos se ha ocupado una técnica llamada \textit{Shuffle}.\\

\textit{Shuffle} consiste en brindar un arreglo con los identificadores de las noticias y un número (nombrado usualmente como semilla), quien generar un nuevo orden en los identificadores de acuerdo a los números pseudo aleatorios que retorna esta función, tomando este como el nueva orden de los textos en el archivo de almacenamiento. Para el desarrollo de esta etapa se ha utilizado la librería \textbf{Shuffle} (ver) con una semilla de 5.\\

Para ilustrar un ejemplo, en el cuadro \ref{box:cp5:ids} se muestra un conjunto de identificadores ordenados por el último número del $id$, al utilizar la función \textit{Shuffle} con una semilla de 2, se genera un nuevo orden el cual es mostrado en el Cuadro \ref{box:cp5:shuffle}.\\


\begin{mygraybox}[label={box:cp5:ids}]{Identificadores de noticias} 
\begin{equation*}
\begin{bmatrix}
1001 & 2002 & 3003 & 4004 & 5005 & 6006 & 7007 & 1008\\ 
\end{bmatrix}
\end{equation*}
\end{mygraybox}

\ \\


\begin{mygraybox}[label={box:cp5:shuffle}]{Nuevo orden de ID's} 
\begin{equation*}
\begin{bmatrix}
5005 & 2002 & 7007 & 3003 & 4004 & 1008 & 6006 & 1001\\
\end{bmatrix}
\end{equation*}
\end{mygraybox}
\ \\

La distribución generada por esta función es almacenada en un archivo, y como último paso se han tomado las primeras 350 noticias de forma manual y se han colocado en un archivo diferente, definido así las noticias de entrenamiento (con 3150) y de prueba (con 350).\\

La cantidad de noticias por sección del conjunto de entrenamiento se muestran en la Figura \ref{fig:cp5:seccionE} y del conjunto de prueba en la Figura \ref{fig:cp5:sitiosP}.



\begin{figure}[H]
\centering
\includegraphics[scale=.55]{imagenes/capitulo5/SeccionesE.png}
\caption{Corpus de entrenamiento}
\label{fig:cp5:seccionE}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=.55]{imagenes/capitulo5/SeccionesP.png}
\caption{Corpus de prueba}
\label{fig:cp5:seccionP}
\end{figure}

%\begin{figure}[h]
%\centering
%\includegraphics[scale=.6]{imagenes/capitulo5/SitiosE.png}
%\caption{Número de noticias pertenecientes a los sitios web}
%\label{fig:cp5:sitiosE}
%\end{figure}



%------------------------------------------------------------------%
\subsection{Entrenamiento}

Para crear un modelo clasificador (el cual resuelve un problema multinomial ver) usando aprendizaje supervisado (ver), se debe construir dos conjuntos etiquetados: entrenamiento para el proceso de aprendizaje y otro de prueba, para medir su precisión. En la sección anterior estos grupos se han formado con noticias de 5 secciones: \textbf{Deportes}, \textbf{Economía}, \textbf{Política}, \textbf{Cultura}, \textbf{Ciencia y tecnología}. Ambos dataset serán usados en 4 algoritmos (seleccionados con base en el estado del arte ver), los cuales son:

\begin{itemize}
	\item \textbf{Naive Bayes} (ver)
	\item \textbf{Regresión logística} (ver)
	\item \textbf{Maquina de soporte vectorial} (ver)
	\item \textbf{Random Forest} (ver)
\end{itemize}

El proceso de entrenamiento consta de 5 pasos los cuales se muestran en la Figura \ref{fig:cp5:entrenamiento}.

\begin{figure}[h]
\centering
\includegraphics[scale=.5]{imagenes/capitulo5/Entrenamiento.png}
\caption{Etapas de entrenamiento}
\label{fig:cp5:entrenamiento}
\end{figure}

El desarrollo se ha implementado en el lenguaje de programación \textbf{Python 3}, ocupando la librería \textbf{scikit learn} quien permite crear instancias de los algoritmos mencionados. Cabe señalar que el objetivo de este trabajo terminal es el entrenamiento de los clasificadores y no su desarrollo.

Como primer paso del entrenamiento, el corpus debe ser traducido del lenguaje español a un conjunto de vectores numéricos que representan la información de las noticias, esta es llamada representación vectorial (ver). Para lograr este objetivo, del \textit{dataset} se deben seleccionar características que definan los elementos del vector numérico.\\

%----------------------------------%
\begin{large}
\textbf{Selección de características}\\
\end{large}

Cada clase de noticias contiene un conjunto de palabras que son comunes en su ámbito, al analizar el léxico usado se observa los tecnicismos usados, por ejemplo ; en la sección deportes se ocupa, fútbol, jugador, ganador ; en política, presidente, corrupción, PRI ; en ciencia y tecnología, investigación, descubrimiento, publicación y así sucesivamente, por lo tanto estos vocablos pueden ser definidos como características que identifican a una sección. En este sentido la selección de características es el proceso de tomar el corpus de noticias e identificar las palabras mas comunes en los artículos.\\ 

Para ejemplificar esta tarea observe el Cuadro \ref{box:cp5:corpus} el cual es un corpus de 4 oraciones. Una vez realizado el proceso de extracción de características se obtiene las palabras relevantes las cuales son mostradas en el Cuadro \ref{box:cp5:caracteristicas}.\\

\begin{mygraybox}[label={box:cp5:corpus}]{Corpus} 
\begin{equation*}
\begin{bmatrix}
Este&es&la&primera&noticia\\
Esta&noticia&es&la&segunda&noticia\\
Y&este&es&la&tercera\\
Es&este&la&primera&noticia&?
\end{bmatrix}
\end{equation*}
\end{mygraybox}

\begin{mygraybox}[label={box:cp5:caracteristicas}]{Selección de características} 
\begin{equation*}
\begin{bmatrix}
es & esta & este & la & noticia & primera & segunda & tercera
\end{bmatrix}
\end{equation*}
\end{mygraybox}

%----------------------------------%

\ \\

\begin{large}
\textbf{Extracción de características}\\
\end{large}

Después de seleccionar las características, se crea un espacio vectorial por cada noticia donde cada elemento del vector representa la presencia o ausencia de una característica (palabra). Cabe mencionar que las características son extraídas de 2 formas, binario(donde 1 representa la presencia de la característica y 0 la ausencia) y por frecuencia (donde se cuenta el número de veces que cada característica aparece). Continuando con el ejemplo del Cuadro \ref{box:cp5:corpus} se extraen las características por frecuencia y el resultado se muestra en el Cuadro \ref{box:cp5:frecuencia}, mientras que el Cuadro \ref{box:cp5:binario} muestra las características extraídas de forma binaria.\\

Para el desarrollo de esta etapa se ha implementado la librería \textbf{CountVectorizer} quien permite generar la selección y extracción de características, ademas esta es la presentación vectorial (de cada noticia) que los algoritmos de clasificación pueden entender y por ende ser entrenados. Cabe destacar que la extracción de características ocupada en este trabajo ha sido de forma binaria,.\\

\begin{mygraybox}[label={box:cp5:frecuencia}]{Extracción por frecuencia} 
\begin{equation*}
\begin{bmatrix}
1 & 0 & 1 & 1 & 1 & 1 & 0 & 0\\ 
1 & 1 & 0 & 1 & 2 & 0 & 1 & 0\\
1 & 0 & 1 & 1 & 1 & 0 & 0 & 1\\
1 & 0 & 1 & 1 & 1 & 1 & 0 & 0\\
\end{bmatrix}
\end{equation*}
\end{mygraybox}

\ \\

\begin{mygraybox}[label={box:cp5:binario}]{Extracción binaría} 
\begin{equation*}
\begin{bmatrix}
1 & 0 & 1 & 1 & 1 & 1 & 0 & 0\\
1 & 1 & 0 & 1 & 1 & 0 & 1 & 0\\
1 & 0 & 1 & 1 & 1 & 0 & 0 & 1\\
1 & 0 & 1 & 1 & 1 & 1 & 0 & 0 
\end{bmatrix}
\end{equation*}
\end{mygraybox}

\ \\
%----------------------------------%

\begin{large}
\textbf{Entrenamiento}\\
\end{large}


El corpus contiene noticias de varias fuentes, en  las cuales la redacción, coherencia, semántica varía, incluso en la edición de una misma noticia, por lo tanto en el entrenamiento se busca generalizar la clasificación de los artículos, analizando el texto como un conjunto de palabras, sin tomar en cuenta la semántica, esta técnica es llamada bolsa de palabras (ver).\\

En este punto del trabajo las noticias están representadas en un espacio vectorial, y serán usadas en el proceso de entrenamiento de los algoritmos: \textbf{Naive Bayes}, \textbf{Regresión logística}, \textbf{Maquina de soporte vectorial}, \textbf{Random Forest}. Cada clasificador reciben como entrada un conjunto de vectores etiquetados y como salida se genera un modelo el cual predice la sección de nuevos textos.\\ 

Para este trabajo se han definido las etiquetas como se muestra en la Tabla \ref{tab:cp5:etiquetas}, esta correspondencia es de secciones de noticias a un un número único.


\begin{table}[h]
\centering
	\begin{tabular}{|l|l|}
	%-----------------------Ecanbezado-----------------------------------%
		\hline
		\multicolumn{1}{| >{\columncolor{white!30!black}}l|}{ \textcolor{myWhite}{\textbf{Sección}} }&
		\multicolumn{1}{| >{\columncolor{white!30!black}}l|}{ \textcolor{myWhite}{\textbf{Etiqueta}} }\\
		\hline
	%------------------------------------------------%

		\textbf{Deportes} & 0 \\
		\hline

		\textbf{Economía} & 1 \\
		\hline
		\textbf{Política} & 2 \\
		\hline

		\textbf{Cultura} & 3 \\
		\hline

		\textbf{Ciencia y tecnología} & 4 \\	
		\hline

	\end{tabular}\\
\caption{Etiquetas de secciones}
\label{tab:cp5:etiquetas}
\end{table}


El desarrollo ha ocupado una instancia de cada algoritmo, para esto se incluye la librería correspondiente de \textbf{sciktlearn}, las cuales se muestran en la Tabla \ref{tab:cp5:librerias}, la entrada al clasificador es el conjunto de espacio vectorial y las etiquetas correspondientes, esto regresa como resultado un modelo que es capas de predecir la sección de noticias, sin que estas estén etiquetadas.

\begin{table}[h]
\centering
	\begin{tabular}{|l|l|}
	%-----------------------Ecanbezado-----------------------------------%
		\hline
		\multicolumn{1}{| >{\columncolor{white!30!black}}l|}{ \textcolor{myWhite}{\textbf{Sección}} }&
		\multicolumn{1}{| >{\columncolor{white!30!black}}l|}{ \textcolor{myWhite}{\textbf{Etiqueta}} }\\
		\hline
	%------------------------------------------------%

		\textbf{Naive Bayes} & MultinomialNB \\
		\hline

		\textbf{Maquina de soporte vectorial} & SVC\\
		\hline

		\textbf{Regresión logísitica} &LogisticRegression\\
		\hline

		\textbf{Ramdon Forest} &  RandomForestClassifier\\
		\hline

	\end{tabular}\\
\caption{Librería de algoritmo}
\label{tab:cp5:librerias}
\end{table}




%----------------------------------%

\begin{large}
\textbf{Validación cruzada}\\
\end{large}

En el proceso de entrenamiento, los clasificadores reciben un conjunto noticias para ser entrenados y otro para realizar pruebas, sin embargo la selección de los artículos puede ser manipulada para obtener un resultado a conveniencia, siendo esto una mala práctica, por esta razón y en con el objetivo de obtener resultados mas robustos se ha implementado un técnica llamada Validación cruzada.\\

Este método consiste en tres pasos: el primero es dividir el corpus en entrenamiento y prueba (este conjunto es llamado pliegue); después se calcula la precisión de la prueba y es almacenado; como último etapa los dos primeros paso son repetidos $n$ veces y para terminar  se calcula el promedio de la precisión. En términos generales este promedio nos brinda mayor confianza en el resultado del entrenamiento de cada clasificador (para ver una explicación mas detallada ver).\\

%----------------------------------%

\begin{large}
\textbf{Variación de parámetros}\\
\end{large}

Como se ha visto, los resultados de la clasificación son medidos por la cantidad de noticias correctas clasificadas, no obstante estos resultados pueden incrementar o decrementar con base a los parámetros ingresados a cada algoritmo.\\

Esta etapa consiste en observar el mejor resultado en la clasificación con los diferentes algoritmos, variando los parámetros y aplicar validación cruzada para medir la precisión. Para ejemplificar este tarea la Figura \ref{fig:cp5:gridbayes} muestra la variación del parámetro \textbf{alpha} en el algoritmo \textbf{Naive bayes} (este parámetro será explicado mas adelante) tomando los valores: 0.5, 1.0, 1.5 y 2.0, aplicando 2 pliegues en la validación cruzada.\\

\begin{figure}[h]
\centering
\includegraphics[scale=.5]{imagenes/capitulo5/gridbayes.png}
\caption{Corpus de entrenamiento}
\label{fig:cp5:gridbayes}
\end{figure}

Se observa que por cada parámetro implementado en la validación cruzada se muestra el resultado \textbf{score}, el cual es el promedio de la precisión de dicha prueba, se puede observar que el mejor resultado ha sido hecha con el $\alpha=0.5$, con un 85\% de precisión.\\


A continuación se explican los parámetros de cada clasificador: \\

%----------------%
%\begin{Text}
\textbf{Naive Bayes}\\
%\end{Text}
 
El parámetro en el cual se varía en este algoritmo, es un escalar llamado \textbf{Alpha} ($\alpha$). Como se explico en el capitulo 3 (ver), un valor numérico es asignado a cada palabra en el corpus con respecto a la frecuencia de aparición en un clase (como se muestra en la ecuación \ref{eq:cp5:naiveBayes}). El valor $\alpha$ evita que dicha ecuación se haga cero, en el caso de la ausencia de una palabra en una clase $c$ (es decir $N_{ck}=0$). \textbf{Alpha} es variado en los valores [$0.5, 1.0, 1.5, 2$].


\begin{equation}
P(t_k|c)=\frac{N_{ck}+\alpha}{N_c+\alpha n}
\label{eq:cp5:naiveBayes}
\end{equation}

%----------------%
%\begin{Text}
\textbf{Regresión logística}\\
%\end{Text}

%Referencia: https://www.kaggle.com/joparga3/2-tuning-parameters-for-logistic-regression
 Este algoritmo está basado en una regresión lineal y el calculo de probabilidades como se explica en el capitulo 3(ver). Existen varias formas de implementar este algoritmo, en este trabajo se ha implementado de dos formas: optimizando la función $l_1$ (ver \ref{eq:cp5:l1}) y minimizando el costo de la función $l2$ (ver \ref{eq:cp5:l2}). Como se observa ambas ecuaciones contiene el escalar $C$, para valores pequeños de este número los valores de los pesos $w$ (la importancia de cada palabra) se decrementa, teniendo así un modelo muy simple (se pierde información), de lo contrario para valores grandes de $C$ la complejidad del modelo aumenta pero se incrementa el ruido.\\

\begin{equation}
\underset{w,c}{min}{\left|w\right|}_1+C\sum_{i=1}^{n}log(exp(-y_i(X_{i}^{T} w+c ))+1)
\label{eq:cp5:l1}
\end{equation}


\begin{equation}
\underset{w,c}{min}{\left|w\right|}_1+C\sum_{i=1}^{n}log(exp(-y_i(X_{i}^{T} w+c ))+1)
\label{eq:cp5:l2}
\end{equation}


Los parámetros de este algoritmo son: $l_1$ y $l_2$ que es la forma de entrenar el algoritmo; $C$ que es el equilibrio entre la simplicidad del algoritmo y la tolerancia de ruido, el cual toma los valores :$[1e-6, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01]$.\\


%----------------%
%\begin{Text}
\textbf{Ramdom forest}\\
%\end{Text}

Este algoritmo está basado en la construcción de conjuntos de arboles de decisión como se explica en el capitulo 3 (ver), donde se tienen que controlar la cantidad de árboles a crear y la profundidad. Cabe señalar que a mas profundidad las características son separadas con mas homogénidad, sin embargo este es un problema, si el árbol creado es completamente homogéneo sufrirá de sobre entrenamiento, hará un buen trabajo clasificando el conjunto de entrenamiento pero lo hará mal con el conjunto de prueba.\\

Los parámetros ocupados en ese algoritmo son; \textbf{n\_estimators} es la cantidad de árboles creados, el cual esta en el rango de : $[50, 100, 500, 1000]$; \textbf{max\_depth} es la profundidad del árbol y ha sido establecida en los rangos: $[50, 100, 500, 1000]$.\\

%----------------%
%\begin{Text}
\textbf{Maquina de soporte vectorial}\\
%\end{Text}

Este algoritmo está basado en clasificar los datos con un margen que equidiste de los tipos de clases (secciones), sin embargo en el caso de que los datos no son separables en la dimensión inicial, se busca encontrar la solución con hiperplanos en dimensiones superiores (ver).\\

Para dividir los datos de forma correcta se busca usar funciones de división la cual es denominada el kernel de la función. En este trabajo se han implementado 3 tipos de kernel: 

\begin{itemize}

	\item \textbf{Lineal}: $<x,x^{'}>$ (ver \ref{fig:cp5:klineal})

	\item \textbf{Polinomial}: $(\gamma <x,x^{'}>+r)^{d}$ (ver Figura \ref{fig:cp5:kpoli})

	\item \textbf{Radial}: $exp(-\gamma {\left|\left| x-x^{'} \right|\right|}^2)$ (ver \ref{fig:cp5:krbf})

\end{itemize}

En general este algoritmo trata de resolver el problema 

\begin{equation}
\underset{w,c,\zeta}{\ min}{\frac{1}{2}w^T  w}m+C\sum_{i=1}^{n}\zeta_i
\label{eq:cp5:msvm}
\end{equation}

Los parámetros de este clasificador es el tipo de kernel ocupado, para el kernel \textbf{polinomial} y \textbf{Radial} se ocupa el parámetro $\gamma$ con los valores: $[1e-4, 1e-5, 1e-6]$. Ademas como se explico en \textbf{regresión logística} se varia el escalar $C$, el cual ajusta la variación y el bías de los datos, a mayor bías se sobre entrena el modelo, es decir que el modelo se ajusta demasiado a los datos de entrenamiento y al brindar el conjunto de entrenamiento la clasificación es deficiente; a mayor varíansa 



\begin{figure}[h]
\centering
	\begin{subfigure}{.5\textwidth}
	\centering
	\includegraphics[scale=0.7]{imagenes/capitulo5/klineal.png}
	\caption{Kernel lineal}
	\label{fig:cp5:klineal}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	\centering
	\includegraphics[scale=0.7]{imagenes/capitulo5/kpoli.png}
	\caption{Kernel polinomial}
	\label{fig:cp5:kpoli}
	\end{subfigure}%
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[scale=0.7]{imagenes/capitulo5/krbf.png}
\caption{Kernel radial}
\label{fig:cp5:krbf}
\end{figure}


Las pruebas de este trabajo terminal han implementado 5 pliegues en la validación cruzada y se ha usado el corpus con 3150 noticias. Los resultados son visualizados en las siguientes tablas, donde la primera columna \textbf{Parámetros} contiene los parámetros variados de cada prueba, las columnas siguientes \textbf{P1}, \textbf{P2}, \textbf{P3}, \textbf{P4} y \textbf{P5} muestran el \textbf{accuracy} obtenido en cada pliegue, la columna \textbf{Promedio} muestra la suma de los \textbf{accuracy} obtenidos divido entre 5 (número de pliegues) y la última columna \textbf{Rank} muestra un número entero el cual indica el \textit{Ranking} de cada prueba, es decir la prueba con rank 1 es el mejor resultado, la prueba con rank 2 es el segundo mejor y así sucesivamente.\\

\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
	\begin{tabular}{|l|l|l|l|l|l|l|l|}
	%-----------------------Ecanbezado-----------------------------------%
		\hline
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Parámetros}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 1}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 2}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 3}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 4}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 5}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Promedio}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Rank}} }
\\  \cline{1-8}
	%--------------

alpha: 0.5&0.8610&0.8576&0.8458&0.8487&0.8424&0.8511&1\\
\hline
alpha: 1&0.8531&0.8497&0.8426&0.8408&0.8360&0.8444&2\\
\hline
alpha: 1.5&0.8531&0.8434&0.8410&0.8392&0.8312&0.8416&3\\
\hline
alpha: 2&0.8531&0.8418&0.8410&0.8392&0.8296&0.8409&4\\
\hline

	\end{tabular}
}
\caption{Naive bayes}
\label{tab:cp5:naive}
\end{table}


\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
	\begin{tabular}{|l|l|l|l|l|l|l|l|}
	%-----------------------Ecanbezado-----------------------------------%
		\hline
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Parámetros}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 1}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 2}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 3}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 4}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 5}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Promedio}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Rank}} }
\\  \cline{1-8}
	%--------------
C: 0.1, penalty: l2, solver: liblinear&0.8768&0.8655&0.8521&0.8710&0.8758&0.8682&1\\
\hline
C: 0.01, penalty: l2, solver: liblinear&0.8705&0.8655&0.8474&0.8742&0.8615&0.8638&2\\
\hline
C: 0.1, penalty: l1, solver: liblinear&0.8515&0.8544&0.8442&0.8424&0.8376&0.8460&3\\
\hline
C: 0.001, penalty: l2, solver: liblinear&0.8436&0.8560&0.8299&0.8392&0.8169&0.8371&4\\
\hline
C: 0.0001, penalty: l2, solver: liblinear&0.7836&0.7832&0.7806&0.7739&0.7723&0.7787&5\\
\hline
C: 0.01, penalty: l1, solver: liblinear&0.6477&0.6297&0.5946&0.6099&0.6099&0.6184&6\\
\hline
C: 1e-05, penalty: l2, solver: liblinear&0.6051&0.6044&0.6057&0.5924&0.5796&0.5974&7\\
\hline
C: 1e-06, penalty: l2, solver: liblinear&0.5261&0.5316&0.5564&0.5223&0.5048&0.5282&8\\
\hline
C: 1e-06, penalty: l1, solver: liblinear&0.2006&0.2009&0.2003&0.2006&0.2006&0.2006&9\\
\hline
C: 1e-05, penalty: l1, solver: liblinear&0.2006&0.2009&0.2003&0.2006&0.2006&0.2006&9\\
\hline
C: 0.0001, penalty: l1, solver: liblinear&0.2006&0.2009&0.2003&0.2006&0.2006&0.2006&9\\
\hline
C: 0.001, penalty: l1, solver: liblinear&0.2006&0.2009&0.2003&0.2006&0.2006&0.2006&9\\
\hline
	\end{tabular}
}
\caption{Regresión logística}
\label{tab:cp5:regresion}
\end{table}


\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
	\begin{tabular}{|l|l|l|l|l|l|l|l|}
	%-----------------------Ecanbezado-----------------------------------%
		\hline
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Parámetros}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 1}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 2}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 3}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 4}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 5}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Promedio}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Rank}} }
\\  \cline{1-8}
	%--------------
max\_depth: 50, n\_estimators: 1000&0.8720&0.8592&0.8537&0.8631&0.8615&0.8619&1\\
\hline
max\_depth: 1000, n\_estimators: 1000&0.8689&0.8639&0.8585&0.8583&0.8583&0.8616&2\\
\hline
max\_depth: 100, n\_estimators: 1000&0.8752&0.8592&0.8569&0.8599&0.8551&0.8613&3\\
\hline
max\_depth: 100, n\_estimators: 500&0.8784&0.8608&0.8601&0.8535&0.8519&0.8609&4\\
\hline
max\_depth: 1000, n\_estimators: 500&0.8720&0.8528&0.8585&0.8551&0.8599&0.8597&5\\
\hline
max\_depth: 50, n\_estimators: 500&0.8705&0.8544&0.8601&0.8535&0.8599&0.8597&6\\
\hline
max\_depth: 500, n\_estimators: 1000&0.8720&0.8655&0.8506&0.8487&0.8583&0.8590&7\\
\hline
max\_depth: 500, n\_estimators: 500&0.8689&0.8544&0.8553&0.8519&0.8615&0.8584&8\\
\hline
max\_depth: 500, n\_estimators: 100&0.8610&0.8608&0.8506&0.8487&0.8662&0.8575&9\\
\hline
max\_depth: 50, n\_estimators: 100&0.8641&0.8528&0.8601&0.8503&0.8455&0.8546&10\\
\hline
max\_depth: 1000, n\_estimators: 100&0.8594&0.8528&0.8474&0.8392&0.8439&0.8485&11\\
\hline
max\_depth: 100, n\_estimators: 100&0.8657&0.8560&0.8394&0.8471&0.8280&0.8473&12\\
\hline
max\_depth: 50, n\_estimators: 50&0.8626&0.8402&0.8267&0.8583&0.8471&0.8470&13\\
\hline
max\_depth: 100, n\_estimators: 50&0.8420&0.8402&0.8283&0.8503&0.8535&0.8429&14\\
\hline
max\_depth: 500, n\_estimators: 50&0.8531&0.8418&0.8331&0.8424&0.8312&0.8403&15\\
\hline
max\_depth: 1000, n\_estimators: 50&0.8578&0.8212&0.8315&0.8392&0.8280&0.8355&16\\
\hline
	\end{tabular}
}
\caption{Random Forest}
\label{tab:cp5:random}
\end{table}


\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
	\begin{tabular}{|l|l|l|l|l|l|l|l|}
	%-----------------------Ecanbezado-----------------------------------%
		\hline
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Parámetros}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 1}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 2}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 3}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 4}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{P 5}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Promedio}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Rank}} }
\\  \cline{1-8}
	%------------------------------------------------%
C: 100, gamma: 0.0001, kernel: rbf&0.8736&0.8639&0.8537&0.8726&0.8822&0.8692&1\\
\hline
C: 1000, gamma: 1e-05, kernel: rbf&0.8752&0.8623&0.8537&0.8710&0.8822&0.8689&2\\
\hline
C: 1000, gamma: 0.0001, kernel: rbf&0.8689&0.8608&0.8506&0.8694&0.8790&0.8657&3\\
\hline
C: 1, gamma: 0.0001, kernel: linear&0.8657&0.8592&0.8410&0.8710&0.8742&0.8622&4\\
\hline
C: 1, gamma: 1e-05, kernel: linear&0.8657&0.8592&0.8410&0.8710&0.8742&0.8622&4\\
\hline
C: 1, gamma: 1e-06, kernel: linear&0.8657&0.8592&0.8410&0.8710&0.8742&0.8622&4\\
\hline
C: 10, gamma: 0.0001, kernel: linear&0.8657&0.8592&0.8394&0.8710&0.8742&0.8619&5\\
\hline
C: 10, gamma: 1e-05, kernel: linear&0.8657&0.8592&0.8394&0.8710&0.8742&0.8619&5\\
\hline
C: 10, gamma: 1e-06, kernel: linear&0.8657&0.8592&0.8394&0.8710&0.8742&0.8619&5\\
\hline
C: 100, gamma: 0.0001, kernel: linear&0.8657&0.8592&0.8394&0.8710&0.8742&0.8619&5\\
\hline
C: 100, gamma: 1e-05, kernel: linear&0.8657&0.8592&0.8394&0.8710&0.8742&0.8619&5\\
\hline
C: 100, gamma: 1e-06, kernel: linear&0.8657&0.8592&0.8394&0.8710&0.8742&0.8619&5\\
\hline
C: 1000, gamma: 0.0001, kernel: linear&0.8657&0.8592&0.8394&0.8710&0.8742&0.8619&5\\
\hline
C: 1000, gamma: 1e-05, kernel: linear&0.8657&0.8592&0.8394&0.8710&0.8742&0.8619&5\\
\hline
C: 1000, gamma: 1e-06, kernel: linear&0.8657&0.8592&0.8394&0.8710&0.8742&0.8619&5\\
\hline
C: 10, gamma: 0.0001, kernel: rbf&0.8641&0.8576&0.8490&0.8678&0.8503&0.8578&6\\
\hline
C: 100, gamma: 1e-05, kernel: rbf&0.8641&0.8576&0.8490&0.8662&0.8519&0.8578&6\\
\hline
C: 1000, gamma: 1e-06, kernel: rbf&0.8641&0.8576&0.8490&0.8662&0.8519&0.8578&6\\
\hline
C: 100, gamma: 1e-06, kernel: rbf&0.6524&0.6772&0.6073&0.6449&0.6608&0.6485&7\\
\hline
C: 10, gamma: 1e-05, kernel: rbf&0.6493&0.6741&0.6073&0.6401&0.6561&0.6454&8\\
\hline
C: 1, gamma: 0.0001, kernel: rbf&0.6193&0.6440&0.5946&0.6115&0.6306&0.6200&9\\
\hline
C: 1, gamma: 0.0001, kernel: poly&0.2038&0.2041&0.2035&0.2038&0.2038&0.2038&10\\
\hline
C: 1, gamma: 1e-05, kernel: poly&0.2038&0.2041&0.2035&0.2038&0.2038&0.2038&10\\
\hline
C: 1, gamma: 1e-05, kernel: rbf&0.2038&0.2041&0.2035&0.2038&0.2038&0.2038&10\\
\hline
C: 1, gamma: 1e-06, kernel: poly&0.2038&0.2041&0.2035&0.2038&0.2038&0.2038&10\\
\hline
C: 1, gamma: 1e-06, kernel: rbf&0.2038&0.2041&0.2035&0.2038&0.2038&0.2038&10\\
\hline
C: 10, gamma: 0.0001, kernel: poly&0.2038&0.2041&0.2035&0.2038&0.2038&0.2038&10\\
\hline
C: 10, gamma: 1e-05, kernel: poly&0.2038&0.2041&0.2035&0.2038&0.2038&0.2038&10\\
\hline
C: 10, gamma: 1e-06, kernel: poly&0.2038&0.2041&0.2035&0.2038&0.2038&0.2038&10\\
\hline
C: 10, gamma: 1e-06, kernel: rbf&0.2038&0.2041&0.2035&0.2038&0.2038&0.2038&10\\
\hline
C: 100, gamma: 0.0001, kernel: poly&0.2038&0.2041&0.2035&0.2038&0.2038&0.2038&10\\
\hline
C: 100, gamma: 1e-05, kernel: poly&0.2038&0.2041&0.2035&0.2038&0.2038&0.2038&10\\
\hline
C: 100, gamma: 1e-06, kernel: poly&0.2038&0.2041&0.2035&0.2038&0.2038&0.2038&10\\
\hline
C: 1000, gamma: 0.0001, kernel: poly&0.2038&0.2041&0.2035&0.2038&0.2038&0.2038&10\\
\hline
C: 1000, gamma: 1e-05, kernel: poly&0.2038&0.2041&0.2035&0.2038&0.2038&0.2038&10\\
\hline
C: 1000, gamma: 1e-06, kernel: poly&0.2038&0.2041&0.2035&0.2038&0.2038&0.2038&10\\
\hline

	\end{tabular}
}
\caption{Maquina de soporte vectorial}
\label{tab:cp5:msv}
\end{table}




%------------------------------------------------------------------%
\subsection{Selección}


Los resultados de la sección anterior ha mostrado que no existe una brecha muy grande en el \textbf{accuracy} obtenido de los mejores parámetros, como se muestra en la siguiente Tabla (\ref{tab:cp5:resultados}).

\begin{table}[H]
\centering
	\begin{tabular}{|l|c|c|}
	%-----------------------Ecanbezado-----------------------------------%
		\hline
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Algoritmo}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Precisión}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Ranking}} }
\\  \cline{1-3}
	%--------------

MSV&0.8692&1\\
\hline
Regresión Logística&0.8619&2\\
\hline
Ramdom Forest&0.8682&3\\
\hline
Naive Bayes&0.8511&4\\
\hline
	\end{tabular}
\caption{Precisión de los mejores parámetros}
\label{tab:cp5:resultados}
\end{table}


El mejor resultado se ha conseguido con el algoritmo \textbf{Maquinas de soporte vectorial} con los parámetros ;$C$=100, $gamma= 0.0001$ y $kernel$= $rbf$ (radial) obteniendo .8619 de \textbf{accuracy}, por esta razón se ha elegido como el clasificador final de este trabajo terminal.

%------------------------------------------------------------------%
\subsection{Pruebas}

Con base en el algoritmo elegido y los parámetros que han conseguido obtener el mejor resultado, se ha hecho una prueba final, la cual consiste en clasificar el corpus de prueba con 350 noticias (creado al final de la sección de pre-procesamiento ver), para calcular la matriz de confusión y obtener las métricas de evaluación. La tabla \ref{tab:cp5:numnoticias} muestra el número de noticias por sección contenidas en el corpus.\\

\begin{table}[H]
\centering
	\begin{tabular}{|l|c|}
	%-----------------------Ecanbezado-----------------------------------%
		\hline
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Sección}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Número de noticias}} }
\\  \cline{1-2}
	%--------------

Deportes& 68\\
\hline
Economía& 58\\
\hline
Política& 69\\
\hline
Cultura& 77\\
\hline
Ciencia y Tecnología& 78\\
\hline
	\end{tabular}
\caption{Número de noticias}
\label{tab:cp5:numnoticias}
\end{table}


La matriz de confusión obtenida se muestra en la tabla \ref{tab:cp5:matriz}, donde las filas representan la clasificación real de la noticia y las columnas lo predicho por el clasificador. Por ejemplo se observa que en la sección \textbf{Deportes} de 68 noticias etiquetadas, 65 fueron clasificadas correctamente, y 3 fueron mal clasificadas en la sección \textbf{ciencia y tecnología} (1 noticia), \textbf{economía} (1 noticia) y \textbf{política} (1 noticia). En la sección de ciencia y tecnología de 78 noticias solo 67 fueron clasificadas de forma acertada, y 11 fueron clasificadas en las distintas secciones: 1 en \textbf{Deportes}, 5 en \textbf{Economía}, 3 en \textbf{Política} y 2 en \textbf{Cultura}.\\

\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
	\begin{tabular}{|l|c|c|c|c|c|}
	%-----------------------Ecanbezado-----------------------------------%
		\hline
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Sección}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Deportes}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Economía}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Política}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Cultura}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Cienci y T}} }
\\  \cline{1-6}
	%--------------

Deportes& 65 & 1 & 1 & 0 & 1\\
\hline
Economía& 1 & 51 & 4 & 1 & 1\\
\hline
Política& 2 & 5 & 59 & 3 & 0\\
\hline
Cultura& 1 & 1 & 1 & 70 & 4\\
\hline
Ciencia y T& 1 & 5 & 3 & 2 & 67\\
\hline
	\end{tabular}
}
\caption{Matriz de confusión}
\label{tab:cp5:matriz}
\end{table}


Con base en la matriz de confusión se ha calculado \textbf{Recall}, \textbf{Fmeasure}, \textbf{Precision} (estos conceptos están explicados en el capitulo 3, ver ). Las métricas son obtenidas por cada sección, como se muestra en la tabla \ref{tab:cp5:metricas}.

\begin{table}[H]
\centering
	\begin{tabular}{|l|c|c|c|}
	%-----------------------Ecanbezado-----------------------------------%
		\hline
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Sección}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Precision}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Recall}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Fmeasuere}} }
\\  \cline{1-4}
	%--------------

Deportes & 0.93 & 0.96 & 0.94\\
\hline
Economía & 0.81 & 0.88 & 0.84\\
\hline
Política & 0.87 & 0.86 & 0.86\\
\hline
Cultura & 0.92 & 0.91 & 0.92\\
\hline
Ciencia y Tecnología & 0.92 & 0.86 & 0.89\\
\hline
	\end{tabular}
\caption{Metricas de evaluación}
\label{tab:cp5:metricas}
\end{table}


Como se puede observar en la tabla \ref{tab:cp5:metricas} las sección con mejor \textbf{Fmeasure} (este valor sostiene una relación entre \textbf{Precision} y \textbf{Rcall}) es \textbf{Deportes} obteniendo 0.94 en la clasificación. Este resultado se puede explicar con base en el vocabulario manejado en la categoría, ya que es muy específico, por ejemplo se usan palabras como: gol, jugador, resultado, anotación. Por lo contrario el \textbf{Fmeasure} mas bajo lo obtuvo \textbf{Economía} con 0.84 (el cual no es un mal resultado), las palabras manejadas por esta sección pueden ser encontradas en otras categorías, generando así confusión en la clasificación.\\

El \textbf{accuracy} obtenido (incluyen todas las secciones) se muestra en la tabla \ref{tab:cp5:accuracy}. En términos generales este resultado ha sido superior que el obtenido en la variación de parámetros, el cual fue \textbf{0.869}. Esta variación muestra que el algoritmo podrá clasificar de una forma eficaz con un resultado cercano a 0.869, es decir podemos asegurar que al menos de 100 noticias que sean clasificadas por el modelo, el 87\% estará correctamente clasificado (o un porcentaje cercano a este).\\


\begin{table}[H]
\centering
	\begin{tabular}{|l|c|}
	%-----------------------Ecanbezado-----------------------------------%
		\hline
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Métrica}} }&
\multicolumn{1}{| >{\columncolor{myBlueChapter}}l|}{ \textcolor{myWhite}{\textbf{Resultado}} }
\\  \cline{1-2}
	%--------------

\textbf{Accuracy}& .89\\
\hline
	\end{tabular}
\caption{Acurracy de la prueba}
\label{tab:cp5:accuracy}
\end{table}

%------------------------------------------------------------------%
\subsection{Persistencia}


Como última etapa de la clasificación, el modelo \textbf{Maquinas de soporte vectorial} debe ser persistido (almacenado) así como las características del corpus. Para esta tarea se ha ocupado el corpus completo de 3500 noticias, del cual se extraen las características, se entrena el modelo y estos conjuntos son almacenados en un archivo ocupando la librería \textbf{pickle}, de esta forma en la siguiente sección el modelo es ocupado como parte de la \textbf{Aplicación web}.



%\subsection{Tokenizar}
%\subsection{Lematizar}
%\subsection{Validación cruzada}
%\subsection{Shuffle}
%\subsection{Grid search}
%\subsection[Resultados]{Selección y resultado del mejor clasificador}
%\subsection[Persistencia]{Persistencia del clasificador}